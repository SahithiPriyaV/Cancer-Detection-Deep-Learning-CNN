{"cells":[{"cell_type":"markdown","metadata":{},"source":["Problem statement: To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis."]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-11T03:45:28.521147Z","iopub.status.busy":"2022-04-11T03:45:28.520528Z","iopub.status.idle":"2022-04-11T03:45:28.541051Z","shell.execute_reply":"2022-04-11T03:45:28.540314Z","shell.execute_reply.started":"2022-04-11T03:45:28.521049Z"}},"source":["### Importing Skin Cancer Data\n","#### To do: Take necessary actions to read the data\n","### Importing all the important libraries"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:38:01.430426Z","iopub.status.busy":"2022-04-13T16:38:01.430146Z","iopub.status.idle":"2022-04-13T16:38:01.436433Z","shell.execute_reply":"2022-04-13T16:38:01.435713Z","shell.execute_reply.started":"2022-04-13T16:38:01.430391Z"},"trusted":true},"outputs":[],"source":["import pathlib\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np # linear algebra\n","import pandas as pd # data processingb\n","import os\n","import PIL\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n","from tensorflow.keras.layers import BatchNormalization\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:38:01.752924Z","iopub.status.busy":"2022-04-13T16:38:01.752667Z","iopub.status.idle":"2022-04-13T16:38:01.759120Z","shell.execute_reply":"2022-04-13T16:38:01.758263Z","shell.execute_reply.started":"2022-04-13T16:38:01.752894Z"},"trusted":true},"outputs":[],"source":["print(tf.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["# Data Reading/Data Understanding"]},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-13T16:39:47.686972Z","iopub.status.busy":"2022-04-13T16:39:47.686267Z","iopub.status.idle":"2022-04-13T16:39:47.692205Z","shell.execute_reply":"2022-04-13T16:39:47.689444Z","shell.execute_reply.started":"2022-04-13T16:39:47.686933Z"},"trusted":true},"outputs":[],"source":["## If you are using the data by mounting the google drive, use the following :\n","## from google.colab import drive\n","## drive.mount('/content/gdrive')\n","\n","##Ref:https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166"]},{"cell_type":"markdown","metadata":{},"source":["This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively."]},{"cell_type":"code","execution_count":59,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-13T16:49:58.147670Z","iopub.status.busy":"2022-04-13T16:49:58.147391Z","iopub.status.idle":"2022-04-13T16:49:58.152758Z","shell.execute_reply":"2022-04-13T16:49:58.151673Z","shell.execute_reply.started":"2022-04-13T16:49:58.147640Z"},"trusted":true},"outputs":[],"source":["# Defining the path for train and test images\n","## Todo: Update the paths of the train and test dataset\n","#Kaggle\n","data_dir_train = pathlib.Path(\"/kaggle/input/dlcnnassignment/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\")\n","data_dir_test = pathlib.Path('/kaggle/input/dlcnnassignment/Skin cancer ISIC The International Skin Imaging Collaboration/Test/')\n","#google\n","# data_dir_train = pathlib.Path(\"/content/drive/MyDrive/CNN_assignment/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\")\n","# data_dir_test = pathlib.Path('/content/drive/MyDrive/CNN_assignment/Skin cancer ISIC The International Skin Imaging Collaboration/Test/')"]},{"cell_type":"code","execution_count":60,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-13T16:49:59.436974Z","iopub.status.busy":"2022-04-13T16:49:59.436712Z","iopub.status.idle":"2022-04-13T16:49:59.468864Z","shell.execute_reply":"2022-04-13T16:49:59.467709Z","shell.execute_reply.started":"2022-04-13T16:49:59.436943Z"},"trusted":true},"outputs":[],"source":["image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n","print(\"Total image count in Train\",image_count_train)\n","image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n","print(\"Total image count in Test\",image_count_test)\n","print(\"Total image count in Dataset\",(image_count_test+image_count_train))"]},{"cell_type":"code","execution_count":61,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-13T16:50:01.714196Z","iopub.status.busy":"2022-04-13T16:50:01.713640Z","iopub.status.idle":"2022-04-13T16:50:01.743472Z","shell.execute_reply":"2022-04-13T16:50:01.742784Z","shell.execute_reply.started":"2022-04-13T16:50:01.714156Z"},"trusted":true},"outputs":[],"source":["# Input data files are available in the read-only \"../input/\" directory\n","import os\n","for dirname, _, filenames in os.walk(data_dir_train):\n","    print(\"Train\", dirname.split(\"/\")[-1], len(filenames))\n","for dirname, _, filenames in os.walk(data_dir_test):\n","    print(\"Test\", dirname.split(\"/\")[-1], len(filenames))"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-11T04:00:54.949267Z","iopub.status.busy":"2022-04-11T04:00:54.948938Z","iopub.status.idle":"2022-04-11T04:00:54.956021Z","shell.execute_reply":"2022-04-11T04:00:54.955133Z","shell.execute_reply.started":"2022-04-11T04:00:54.949214Z"}},"source":["Hence output number of classes or neurons is 9"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Creation\n","\n","Create train & validation dataset from the train directory with a batch size of 32. Also, make sure you resize your images to 180*180."]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-11T04:07:44.232425Z","iopub.status.busy":"2022-04-11T04:07:44.231932Z","iopub.status.idle":"2022-04-11T04:07:44.235933Z","shell.execute_reply":"2022-04-11T04:07:44.235018Z","shell.execute_reply.started":"2022-04-11T04:07:44.232392Z"}},"source":["### Load using keras.preprocessing\n","\n","Let's load these images off disk using the helpful image_dataset_from_directory utility.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Create a dataset\n","\n","Define some parameters for the loader:"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:39:59.693153Z","iopub.status.busy":"2022-04-13T16:39:59.692900Z","iopub.status.idle":"2022-04-13T16:39:59.697677Z","shell.execute_reply":"2022-04-13T16:39:59.696717Z","shell.execute_reply.started":"2022-04-13T16:39:59.693125Z"},"trusted":true},"outputs":[],"source":["batch_size = 32\n","img_height = 180\n","img_width = 180"]},{"cell_type":"markdown","metadata":{},"source":["Use 80% of the images for training, and 20% for validation."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:40:09.025513Z","iopub.status.busy":"2022-04-13T16:40:09.024943Z","iopub.status.idle":"2022-04-13T16:40:11.540659Z","shell.execute_reply":"2022-04-13T16:40:11.539194Z","shell.execute_reply.started":"2022-04-13T16:40:09.025473Z"},"trusted":true},"outputs":[],"source":["## Write your train dataset here\n","## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n","## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\n","train_ds =  tf.keras.preprocessing.image_dataset_from_directory(\n","    data_dir_train,\n","    seed=123,\n","    validation_split= 0.2, # 20% for validation\n","    subset= 'training',\n","    image_size=(img_height,img_width),\n","    batch_size = batch_size\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:40:11.542711Z","iopub.status.busy":"2022-04-13T16:40:11.542465Z","iopub.status.idle":"2022-04-13T16:40:11.551335Z","shell.execute_reply":"2022-04-13T16:40:11.550645Z","shell.execute_reply.started":"2022-04-13T16:40:11.542673Z"},"trusted":true},"outputs":[],"source":["train_ds"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:40:11.553791Z","iopub.status.busy":"2022-04-13T16:40:11.553128Z","iopub.status.idle":"2022-04-13T16:40:11.679760Z","shell.execute_reply":"2022-04-13T16:40:11.678658Z","shell.execute_reply.started":"2022-04-13T16:40:11.553678Z"},"trusted":true},"outputs":[],"source":["## Write your validation dataset here\n","## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n","## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir_train,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:40:11.682859Z","iopub.status.busy":"2022-04-13T16:40:11.682275Z","iopub.status.idle":"2022-04-13T16:40:11.688243Z","shell.execute_reply":"2022-04-13T16:40:11.687400Z","shell.execute_reply.started":"2022-04-13T16:40:11.682791Z"},"trusted":true},"outputs":[],"source":["val_ds"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:40:11.690144Z","iopub.status.busy":"2022-04-13T16:40:11.689676Z","iopub.status.idle":"2022-04-13T16:40:11.698025Z","shell.execute_reply":"2022-04-13T16:40:11.697094Z","shell.execute_reply.started":"2022-04-13T16:40:11.690105Z"},"trusted":true},"outputs":[],"source":["# List out all the classes of skin cancer and store them in a list. \n","# You can find the class names in the class_names attribute on these datasets. \n","# These correspond to the directory names in alphabetical order.\n","class_names = train_ds.class_names\n","print(class_names)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset visualisation\n","Create a code to visualize one instance of all the nine classes present in the dataset \n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-11T05:32:47.334681Z","iopub.status.busy":"2022-04-11T05:32:47.334353Z","iopub.status.idle":"2022-04-11T05:32:47.338978Z","shell.execute_reply":"2022-04-11T05:32:47.338285Z","shell.execute_reply.started":"2022-04-11T05:32:47.334631Z"}},"source":["### Visualize the data\n","#### Todo, create a code to visualize one instance of all the nine classes present in the dataset"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:40:26.012259Z","iopub.status.busy":"2022-04-13T16:40:26.011520Z","iopub.status.idle":"2022-04-13T16:40:28.236842Z","shell.execute_reply":"2022-04-13T16:40:28.236199Z","shell.execute_reply.started":"2022-04-13T16:40:26.012221Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","### your code goes here, you can use training or validation data to visualize\n","plt.figure(figsize=(10, 10))\n","for im, lab in train_ds.take(1):\n","    for i in range(9):\n","        ax = plt.subplot(3, 3, i + 1)\n","        plt.imshow(im[i].numpy().astype(\"uint8\"))\n","        plt.title(class_names[lab[i]])\n","        plt.axis(\"off\")\n","        \n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:40:28.238598Z","iopub.status.busy":"2022-04-13T16:40:28.238243Z","iopub.status.idle":"2022-04-13T16:40:29.969030Z","shell.execute_reply":"2022-04-13T16:40:29.968311Z","shell.execute_reply.started":"2022-04-13T16:40:28.238565Z"},"trusted":true},"outputs":[],"source":["ds=train_ds.take(1) #divides dataset to batches of 32, generates first 32 batch\n","print(type(ds))\n","print(ds)\n","for im, lab in ds:\n","    print(im.shape)\n","# print(im[0].numpy().astype(\"uint8\"),im[0].numpy().astype(\"uint8\").shape,class_names[lab[0]])\n","im[0]"]},{"cell_type":"markdown","metadata":{},"source":["The `image_batch` is a tensor of the shape `(32, 180, 180, 3)`. This is a batch of 32 images of shape `180x180x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.\n","\n","`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch.\n","\n","`Dataset.prefetch()` overlaps data preprocessing and model execution while training."]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:40:47.092051Z","iopub.status.busy":"2022-04-13T16:40:47.091439Z","iopub.status.idle":"2022-04-13T16:40:47.099962Z","shell.execute_reply":"2022-04-13T16:40:47.099204Z","shell.execute_reply.started":"2022-04-13T16:40:47.092008Z"},"trusted":true},"outputs":[],"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:40:48.091489Z","iopub.status.busy":"2022-04-13T16:40:48.091195Z","iopub.status.idle":"2022-04-13T16:40:48.098469Z","shell.execute_reply":"2022-04-13T16:40:48.097639Z","shell.execute_reply.started":"2022-04-13T16:40:48.091452Z"},"trusted":true},"outputs":[],"source":["train_ds"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:40:56.618103Z","iopub.status.busy":"2022-04-13T16:40:56.617322Z","iopub.status.idle":"2022-04-13T16:40:56.624141Z","shell.execute_reply":"2022-04-13T16:40:56.623455Z","shell.execute_reply.started":"2022-04-13T16:40:56.618058Z"},"trusted":true},"outputs":[],"source":["val_ds"]},{"cell_type":"markdown","metadata":{},"source":["## Model Building & training : \n","Create a CNN model, which can accurately detect 9 classes present in the dataset. While building the model rescale images to normalize pixel values between (0,1).\n","Choose an appropriate optimiser and loss function for model training\n","Train the model for ~20 epochs\n","Write your findings after the model fit, see if there is evidence of model overfit or underfit"]},{"cell_type":"markdown","metadata":{},"source":["### Create the model\n","#### Todo: Create a CNN model, which can accurately detect 9 classes present in the dataset. Use ```layers.experimental.preprocessing.Rescaling``` to normalize pixel values between (0,1). The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network. Here, it is good to standardize values to be in the `[0, 1]`"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:41:03.499673Z","iopub.status.busy":"2022-04-13T16:41:03.498965Z","iopub.status.idle":"2022-04-13T16:41:03.518968Z","shell.execute_reply":"2022-04-13T16:41:03.518270Z","shell.execute_reply.started":"2022-04-13T16:41:03.499635Z"},"trusted":true},"outputs":[],"source":["### Your code goes here\n","normalized_layers = tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(180, 180, 3))\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:41:04.509970Z","iopub.status.busy":"2022-04-13T16:41:04.509151Z","iopub.status.idle":"2022-04-13T16:41:17.755624Z","shell.execute_reply":"2022-04-13T16:41:17.753301Z","shell.execute_reply.started":"2022-04-13T16:41:04.509916Z"},"trusted":true},"outputs":[],"source":["#Mapping values to images\n","normalized_ds = train_ds.map(lambda x, y: (normalized_layers(x), y))\n","image_batch, labels_batch = next(iter(normalized_ds))\n","first_image = image_batch[0]\n","# Notice the pixel values are now in `[0,1]`.\n","print(np.min(first_image), np.max(first_image))\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:41:17.757864Z","iopub.status.busy":"2022-04-13T16:41:17.757348Z","iopub.status.idle":"2022-04-13T16:41:17.764348Z","shell.execute_reply":"2022-04-13T16:41:17.763628Z","shell.execute_reply.started":"2022-04-13T16:41:17.757824Z"},"trusted":true},"outputs":[],"source":["print(first_image)"]},{"cell_type":"markdown","metadata":{},"source":["# Basic Model 1"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:41:22.883073Z","iopub.status.busy":"2022-04-13T16:41:22.882810Z","iopub.status.idle":"2022-04-13T16:41:22.981259Z","shell.execute_reply":"2022-04-13T16:41:22.980538Z","shell.execute_reply.started":"2022-04-13T16:41:22.883043Z"},"trusted":true},"outputs":[],"source":["\n","model = Sequential([\n","                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n","])\n","model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu', input_shape = (180, 180, 3)))\n","model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.25))\n","\n","\n","model.add(Flatten())\n","model.add(Dense(512, activation='relu')) # fully connected\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(9, activation = \"softmax\"))"]},{"cell_type":"markdown","metadata":{},"source":["### Compile the model\n","Choose an appropirate optimiser and loss function for model training "]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:41:28.844433Z","iopub.status.busy":"2022-04-13T16:41:28.844154Z","iopub.status.idle":"2022-04-13T16:41:28.860161Z","shell.execute_reply":"2022-04-13T16:41:28.859354Z","shell.execute_reply.started":"2022-04-13T16:41:28.844401Z"},"trusted":true},"outputs":[],"source":["### Todo, choose an appropirate optimiser and loss function\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:41:35.693842Z","iopub.status.busy":"2022-04-13T16:41:35.693274Z","iopub.status.idle":"2022-04-13T16:41:35.705495Z","shell.execute_reply":"2022-04-13T16:41:35.704658Z","shell.execute_reply.started":"2022-04-13T16:41:35.693800Z"},"trusted":true},"outputs":[],"source":["# View the summary of all layers\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### Train the model"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:42:05.842718Z","iopub.status.busy":"2022-04-13T16:42:05.842239Z","iopub.status.idle":"2022-04-13T16:43:10.450104Z","shell.execute_reply":"2022-04-13T16:43:10.449420Z","shell.execute_reply.started":"2022-04-13T16:42:05.842664Z"},"trusted":true},"outputs":[],"source":["epochs = 20\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Visualizing training results"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:44:03.552060Z","iopub.status.busy":"2022-04-13T16:44:03.551790Z","iopub.status.idle":"2022-04-13T16:44:03.824078Z","shell.execute_reply":"2022-04-13T16:44:03.823409Z","shell.execute_reply.started":"2022-04-13T16:44:03.552031Z"},"trusted":true},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Todo: Write your findings after the model fit, see if there is an evidence of model overfit or underfit\n","\n","Yes, Model is clearly overestimating/overfitting\n","\n","(\n","Epoch 20/20\n","\n","56/56 [==============================] - 2s 31ms/step - loss: 0.7274 - accuracy: 0.7494 - val_loss: 1.6475 - val_accuracy: 0.5481)\n","\n","as train accuracy is larger than validation accuracy.\n","After 5 epoch validation staled at 50% kept oscillating arounf similarly the validation loss.\n","\n","Reason being model had been trained/learnt more parameters. It is recommended to maintain model generalized to perform some of the following steps:\n","1. Dropouts.\n","2.Regularization(l1 and l2)\n","3. Data Augmentation.(To increase training data)\n","Since suggested to follow let us for now choose Data Augmentation."]},{"cell_type":"markdown","metadata":{},"source":["### Data Augmentation"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","As there are many Augmentation methods: ImageDataNet, Keras.PreProcessing, Imgaug etc,\n","\n","Let us fornow start with basic coarse tuned augmentation strategy-Keras Preprocessing Layers."]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:08.144090Z","iopub.status.busy":"2022-04-13T16:46:08.143308Z","iopub.status.idle":"2022-04-13T16:46:08.242704Z","shell.execute_reply":"2022-04-13T16:46:08.242016Z","shell.execute_reply.started":"2022-04-13T16:46:08.144050Z"},"trusted":true},"outputs":[],"source":["# Todo, after you have analysed the model fit history for presence of underfit or overfit, choose an appropriate data augumentation strategy. \n","# Your code goes here\n","\n","data_augument = keras.Sequential([\n","                             \n","                             layers.experimental.preprocessing.RandomFlip(mode=\"horizontal_and_vertical\",input_shape=(180,180,3)),\n","                             layers.experimental.preprocessing.RandomRotation(0.2, fill_mode='reflect')\n","#                              layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3), fill_mode='reflect')\n","])"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:09.877132Z","iopub.status.busy":"2022-04-13T16:46:09.876854Z","iopub.status.idle":"2022-04-13T16:46:09.893752Z","shell.execute_reply":"2022-04-13T16:46:09.893043Z","shell.execute_reply.started":"2022-04-13T16:46:09.877101Z"},"trusted":true},"outputs":[],"source":["# Todo, visualize how your augmentation strategy works for one instance of training image.\n","# Your code goes here\n","ds=train_ds.take(1) #divides dataset to batches of 32, generates first 32 batch\n","print(type(ds))\n","print(ds)\n","for im, lab in ds:\n","    print(im.shape)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:11.631089Z","iopub.status.busy":"2022-04-13T16:46:11.630822Z","iopub.status.idle":"2022-04-13T16:46:11.868035Z","shell.execute_reply":"2022-04-13T16:46:11.867355Z","shell.execute_reply.started":"2022-04-13T16:46:11.631058Z"},"trusted":true},"outputs":[],"source":["img=im[0].numpy().astype(\"uint8\")\n","plt.imshow(img)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:13.840217Z","iopub.status.busy":"2022-04-13T16:46:13.839942Z","iopub.status.idle":"2022-04-13T16:46:13.845438Z","shell.execute_reply":"2022-04-13T16:46:13.844604Z","shell.execute_reply.started":"2022-04-13T16:46:13.840185Z"},"trusted":true},"outputs":[],"source":["img.shape"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:15.137705Z","iopub.status.busy":"2022-04-13T16:46:15.137025Z","iopub.status.idle":"2022-04-13T16:46:15.144788Z","shell.execute_reply":"2022-04-13T16:46:15.144022Z","shell.execute_reply.started":"2022-04-13T16:46:15.137661Z"},"trusted":true},"outputs":[],"source":["img"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:16.384182Z","iopub.status.busy":"2022-04-13T16:46:16.383428Z","iopub.status.idle":"2022-04-13T16:46:16.397324Z","shell.execute_reply":"2022-04-13T16:46:16.396436Z","shell.execute_reply.started":"2022-04-13T16:46:16.384141Z"},"trusted":true},"outputs":[],"source":["image_resized_cast = tf.cast(tf.expand_dims(img, 0), tf.float32)\n","image_resized_cast.shape\n"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:17.867156Z","iopub.status.busy":"2022-04-13T16:46:17.866532Z","iopub.status.idle":"2022-04-13T16:46:18.576292Z","shell.execute_reply":"2022-04-13T16:46:18.575601Z","shell.execute_reply.started":"2022-04-13T16:46:17.867120Z"},"trusted":true},"outputs":[],"source":["#Individual Image\n","plt.figure(figsize=(10, 10))\n","for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(data_augument(image_resized_cast)[0].numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:24.163710Z","iopub.status.busy":"2022-04-13T16:46:24.163065Z","iopub.status.idle":"2022-04-13T16:46:24.199577Z","shell.execute_reply":"2022-04-13T16:46:24.198835Z","shell.execute_reply.started":"2022-04-13T16:46:24.163656Z"},"trusted":true},"outputs":[],"source":["augmented_image = data_augument(image_resized_cast)\n","augmented_image"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:25.459239Z","iopub.status.busy":"2022-04-13T16:46:25.458783Z","iopub.status.idle":"2022-04-13T16:46:25.704454Z","shell.execute_reply":"2022-04-13T16:46:25.703755Z","shell.execute_reply.started":"2022-04-13T16:46:25.459202Z"},"trusted":true},"outputs":[],"source":["rescale= keras.Sequential([\n","    layers.experimental.preprocessing.Rescaling(1./255)\n","])\n","rescaled_image=rescale(image_resized_cast)[0].numpy()\n","plt.imshow(rescaled_image)"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:27.451542Z","iopub.status.busy":"2022-04-13T16:46:27.450988Z","iopub.status.idle":"2022-04-13T16:46:27.459641Z","shell.execute_reply":"2022-04-13T16:46:27.458925Z","shell.execute_reply.started":"2022-04-13T16:46:27.451502Z"},"trusted":true},"outputs":[],"source":["rescaled_image"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:28.755983Z","iopub.status.busy":"2022-04-13T16:46:28.755296Z","iopub.status.idle":"2022-04-13T16:46:29.591921Z","shell.execute_reply":"2022-04-13T16:46:29.591252Z","shell.execute_reply.started":"2022-04-13T16:46:28.755949Z"},"trusted":true},"outputs":[],"source":["#For batch\n","plt.figure(figsize=(12, 12))\n","for images, labels in train_ds.take(1):\n","    for i in range(9):\n","        ax = plt.subplot(3, 3, i + 1)\n","        plt.imshow(data_augument(images)[0].numpy().astype(\"uint8\"))\n","        plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{},"source":["### Todo:\n","### Create the model, compile and train the model"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:55.096338Z","iopub.status.busy":"2022-04-13T16:46:55.096080Z","iopub.status.idle":"2022-04-13T16:46:55.265295Z","shell.execute_reply":"2022-04-13T16:46:55.264532Z","shell.execute_reply.started":"2022-04-13T16:46:55.096310Z"},"trusted":true},"outputs":[],"source":["## You can use Dropout layer if there is an evidence of overfitting in your findings\n","## Your code goes here\n","model = Sequential([\n","                    data_augument,\n","                    rescale\n","])\n","model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu', input_shape = (180, 180, 3)))\n","model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(512, activation='relu')) # fully connected\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(9, activation = \"softmax\"))\n"]},{"cell_type":"markdown","metadata":{},"source":["### Compiling the model"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:46:58.451068Z","iopub.status.busy":"2022-04-13T16:46:58.450783Z","iopub.status.idle":"2022-04-13T16:46:58.472009Z","shell.execute_reply":"2022-04-13T16:46:58.471323Z","shell.execute_reply.started":"2022-04-13T16:46:58.451034Z"},"trusted":true},"outputs":[],"source":["## Your code goes here\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-12T13:32:23.829562Z","iopub.status.busy":"2022-04-12T13:32:23.828997Z","iopub.status.idle":"2022-04-12T14:38:48.835191Z","shell.execute_reply":"2022-04-12T14:38:48.83371Z","shell.execute_reply.started":"2022-04-12T13:32:23.829527Z"}},"source":["### Training the model"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:47:08.846664Z","iopub.status.busy":"2022-04-13T16:47:08.846348Z","iopub.status.idle":"2022-04-13T16:48:04.605652Z","shell.execute_reply":"2022-04-13T16:48:04.604954Z","shell.execute_reply.started":"2022-04-13T16:47:08.846633Z"},"trusted":true},"outputs":[],"source":["## Your code goes here, note: train your model for 20 epochs\n","epochs=25\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Visualizing the results"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:48:18.236783Z","iopub.status.busy":"2022-04-13T16:48:18.236494Z","iopub.status.idle":"2022-04-13T16:48:18.529929Z","shell.execute_reply":"2022-04-13T16:48:18.529225Z","shell.execute_reply.started":"2022-04-13T16:48:18.236754Z"},"trusted":true},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Todo: Write your findings after the model fit, see if there is an evidence of model overfit or underfit. Do you think there is some improvement now as compared to the previous model run?\n","\n","Though Model overfit has been addressed, the accuracy of model reduced,\n","\n","(\n","Epoch 25/25\n","\n","56/56 [==============================] - 2s 34ms/step - loss: 1.3128 - accuracy: 0.5363 - val_loss: 1.3443 - val_accuracy: 0.5347\n","\n","therefore might be many possible reasons among which class imablance is one of reason. \n","As images count which observed for each individual category is different model may be more biased. \n","\n","Hence let us use some techniques to maintain similar proportion image rate among the 9 categories.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Class Imbalance"]},{"cell_type":"markdown","metadata":{},"source":["#### **Todo:** Find the distribution of classes in the training dataset.\n","#### **Context:** Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data."]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:51:23.368188Z","iopub.status.busy":"2022-04-13T16:51:23.367928Z","iopub.status.idle":"2022-04-13T16:51:23.697241Z","shell.execute_reply":"2022-04-13T16:51:23.696593Z","shell.execute_reply.started":"2022-04-13T16:51:23.368159Z"},"trusted":true},"outputs":[],"source":["## Your code goes here.\n","class_names=[]\n","file_count=[]\n","for dirname, _, filenames in os.walk(data_dir_train):\n","    if dirname.split(\"/\")[-1]!=\"Train\":\n","        print(\"Train\", dirname.split(\"/\")[-1], len(filenames))\n","        class_names.append(dirname.split(\"/\")[-1])\n","        file_count.append(len(filenames))\n","    \n","\n","plt.figure(figsize=(20,8))\n","plt.subplot(1,2,1)\n","plt.bar(class_names,file_count)\n","plt.xticks(fontsize=8, rotation=90)\n","plt.subplot(1,2,2)\n","\n","plt.pie(file_count, labels = class_names,autopct='%.2f')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### **Todo:** Write your findings here: \n","#### - Which class has the least number of samples?\n","#### - Which classes dominate the data in terms proportionate number of samples?\n","\n","\n","As from above bar and pie charts we can infer following points:\n","\n","pigmented benign keratosis has Highest sample proportion rate--20.63%\n","\n","seborrheic keratosis has least sample rate--3.44%\n"]},{"cell_type":"markdown","metadata":{},"source":["#### **Todo:** Rectify the class imbalance\n","#### **Context:** You can use a python package known as `Augmentor` (https://augmentor.readthedocs.io/en/master/) to add more samples across all classes so that none of the classes have very few samples."]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:53:24.896103Z","iopub.status.busy":"2022-04-13T16:53:24.895453Z","iopub.status.idle":"2022-04-13T16:53:24.901169Z","shell.execute_reply":"2022-04-13T16:53:24.900109Z","shell.execute_reply.started":"2022-04-13T16:53:24.896054Z"},"trusted":true},"outputs":[],"source":["\n","class_names=['pigmented benign keratosis',\n"," 'melanoma',\n"," 'vascular lesion',\n"," 'actinic keratosis',\n"," 'squamous cell carcinoma',\n"," 'basal cell carcinoma',\n"," 'seborrheic keratosis',\n"," 'dermatofibroma',\n"," 'nevus']"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:53:26.341152Z","iopub.status.busy":"2022-04-13T16:53:26.340882Z","iopub.status.idle":"2022-04-13T16:53:37.158508Z","shell.execute_reply":"2022-04-13T16:53:37.157666Z","shell.execute_reply.started":"2022-04-13T16:53:26.341122Z"},"trusted":true},"outputs":[],"source":["!pip install Augmentor"]},{"cell_type":"markdown","metadata":{},"source":["To use `Augmentor`, the following general procedure is followed:\n","\n","1. Instantiate a `Pipeline` object pointing to a directory containing your initial image data set.<br>\n","2. Define a number of operations to perform on this data set using your `Pipeline` object.<br>\n","3. Execute these operations by calling the `Pipelineâ€™s` `sample()` method."]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:54:02.611905Z","iopub.status.busy":"2022-04-13T16:54:02.611628Z","iopub.status.idle":"2022-04-13T16:59:20.583564Z","shell.execute_reply":"2022-04-13T16:59:20.582900Z","shell.execute_reply.started":"2022-04-13T16:54:02.611876Z"},"trusted":true},"outputs":[],"source":["path_to_training_dataset=str(data_dir_train)\n","import Augmentor\n","for i in class_names:\n","    p = Augmentor.Pipeline( source_directory=pathlib.Path(path_to_training_dataset +\"/\"+ i),output_directory=\"/output/\"+i)\n","    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n","    p.sample(500)"]},{"cell_type":"markdown","metadata":{},"source":["Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images."]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T16:59:20.586611Z","iopub.status.busy":"2022-04-13T16:59:20.585004Z","iopub.status.idle":"2022-04-13T16:59:20.610354Z","shell.execute_reply":"2022-04-13T16:59:20.609364Z","shell.execute_reply.started":"2022-04-13T16:59:20.586570Z"},"trusted":true},"outputs":[],"source":["# image_count_train = len(list(data_dir_train.glob('/output/*.jpg')))\n","# print(image_count_train)\n","import os\n","for dirname, _, filenames in os.walk('/output/'):\n","    print(\"Train\", dirname.split(\"/\")[-1], len(filenames))"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T17:06:12.556319Z","iopub.status.busy":"2022-04-13T17:06:12.555867Z","iopub.status.idle":"2022-04-13T17:06:12.646526Z","shell.execute_reply":"2022-04-13T17:06:12.645718Z","shell.execute_reply.started":"2022-04-13T17:06:12.556280Z"},"trusted":true},"outputs":[],"source":["### Lets see the distribution of augmented data after adding new images to the original training data.\n","from glob import glob\n","path_list = [x for x in os.walk('/output/')]\n","path_list\n","\n","# dataframe_dict_new = dict(zip(path_list_new, lesion_list_new))\n","# df2 = pd.DataFrame(list(dataframe_dict_new.items()),columns = ['Path','Label'])\n","# new_df = original_df.append(df2)\n","# new_df['Label'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["So, now we have added 500 images to all the classes to maintain some class balance. We can add more images as we want to improve training process."]},{"cell_type":"markdown","metadata":{},"source":["#### **Todo**: Train the model on the data created using Augmentor"]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T17:46:47.820619Z","iopub.status.busy":"2022-04-13T17:46:47.820321Z","iopub.status.idle":"2022-04-13T17:46:47.824424Z","shell.execute_reply":"2022-04-13T17:46:47.823525Z","shell.execute_reply.started":"2022-04-13T17:46:47.820590Z"},"trusted":true},"outputs":[],"source":["batch_size = 32\n","img_height = 180\n","img_width = 180"]},{"cell_type":"markdown","metadata":{},"source":["#### **Todo:** Create a training dataset"]},{"cell_type":"code","execution_count":107,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T17:46:50.592064Z","iopub.status.busy":"2022-04-13T17:46:50.591806Z","iopub.status.idle":"2022-04-13T17:46:50.728109Z","shell.execute_reply":"2022-04-13T17:46:50.726862Z","shell.execute_reply.started":"2022-04-13T17:46:50.592035Z"},"trusted":true},"outputs":[],"source":["data_dir_train=\"/output/\"\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir_train,\n","  seed=123,\n","  validation_split = 0.2,\n","  subset= 'training',## Todo choose the correct parameter value, so that only training data is refered to,,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["#### **Todo:** Create a validation dataset"]},{"cell_type":"code","execution_count":108,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T17:46:54.110880Z","iopub.status.busy":"2022-04-13T17:46:54.108550Z","iopub.status.idle":"2022-04-13T17:46:54.363990Z","shell.execute_reply":"2022-04-13T17:46:54.363310Z","shell.execute_reply.started":"2022-04-13T17:46:54.110831Z"},"trusted":true},"outputs":[],"source":["val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir_train,\n","  seed=123,\n","  validation_split = 0.2,\n","  subset = 'validation', ## Todo choose the correct parameter value, so that only validation data is refered to,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["#### **Todo:** Create your model (make sure to include normalization)"]},{"cell_type":"code","execution_count":109,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T17:47:03.869084Z","iopub.status.busy":"2022-04-13T17:47:03.868320Z","iopub.status.idle":"2022-04-13T17:47:03.980286Z","shell.execute_reply":"2022-04-13T17:47:03.979619Z","shell.execute_reply.started":"2022-04-13T17:47:03.869044Z"},"trusted":true},"outputs":[],"source":["model = Sequential([\n","                    \n","                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n","])\n","model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu', input_shape = (180, 180, 3)))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.25))\n","\n","\n","model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.35))\n","\n","model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2,2)))\n","\n","model.add(Dropout(0.4))\n","\n","model.add(Flatten())\n","model.add(Dense(512, activation='relu')) # fully connected\n","\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(9, activation = \"softmax\"))\n"]},{"cell_type":"markdown","metadata":{},"source":["#### **Todo:** Compile your model (Choose optimizer and loss function appropriately)"]},{"cell_type":"code","execution_count":110,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T17:47:13.434040Z","iopub.status.busy":"2022-04-13T17:47:13.433768Z","iopub.status.idle":"2022-04-13T17:47:13.455838Z","shell.execute_reply":"2022-04-13T17:47:13.455165Z","shell.execute_reply.started":"2022-04-13T17:47:13.434010Z"},"trusted":true},"outputs":[],"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["#### **Todo:**  Train your model"]},{"cell_type":"code","execution_count":111,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T17:47:23.265169Z","iopub.status.busy":"2022-04-13T17:47:23.264905Z","iopub.status.idle":"2022-04-13T18:02:32.624180Z","shell.execute_reply":"2022-04-13T18:02:32.623407Z","shell.execute_reply.started":"2022-04-13T17:47:23.265139Z"},"trusted":true},"outputs":[],"source":["epochs=55\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### **Todo:**  Visualize the model results"]},{"cell_type":"code","execution_count":112,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T18:02:41.092958Z","iopub.status.busy":"2022-04-13T18:02:41.092700Z","iopub.status.idle":"2022-04-13T18:02:41.400005Z","shell.execute_reply":"2022-04-13T18:02:41.399328Z","shell.execute_reply.started":"2022-04-13T18:02:41.092930Z"},"trusted":true},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Observations\n","\n","Though training and validation accuracy is 89% and  78% at 55 epoch as below\n","Epoch 55/55\n","\n","113/113 [==============================] - 12s 96ms/step - loss: 0.2986 - accuracy: 0.8914 - val_loss: 1.0442 - val_accuracy: 0.7856\n","    \n","From The train and validation accuracy, loss graphs and logs of model fit , we may conclude we certain that there are many other saddle points that it may converge, but at epoch 38-42 we may conclude to be early stopping point as model found to have significant accuracy at mimnum loss with acceptable variation between test and validation data.\n","\n","As below:\n","Epoch 38/55\n","\n","113/113 [==============================] - 12s 101ms/step - loss: 0.4759 - accuracy: 0.8214 - val_loss: 0.7690 - val_accuracy: 0.7933\n","Epoch 39/55\n","\n","113/113 [==============================] - 12s 98ms/step - loss: 0.5008 - accuracy: 0.8114 - val_loss: 0.8947 - val_accuracy: 0.7578\n","Epoch 40/55\n","113/113 [==============================] - 12s 101ms/step - loss: 0.4350 - accuracy: 0.8383 - val_loss: 0.9823 - val_accuracy: 0.7500\n","\n","Epoch 41/55\n","113/113 [==============================] - 12s 94ms/step - loss: 0.4349 - accuracy: 0.8394 - val_loss: 0.7954 - val_accuracy: 0.7711\n","                \n","                \n","Therefore we may model above very significant however we may experiment with adding Regularization and variating droput  percentage similary other  hyperparameters, cross validation and optimisation methods to obtain many more high significant and performant models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
